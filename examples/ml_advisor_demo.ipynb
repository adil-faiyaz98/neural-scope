{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c60da2c8",
   "metadata": {},
   "source": [
    "# ML Advisor: Detecting Inefficiencies in Machine Learning Code\n",
    "\n",
    "This notebook demonstrates how to use the ML Advisor tool to detect and fix common inefficiencies in machine learning code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4ab8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ML Advisor extension\n",
    "%load_ext neural_scope.advanced_analysis.ml_advisor.jupyter_extension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204b7ba0",
   "metadata": {},
   "source": [
    "## Demonstration of Common ML Inefficiencies\n",
    "\n",
    "Let's look at some common inefficiencies in ML code and how the advisor can detect and fix them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ae4411",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ml_advisor\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Define a simple model\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(10, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Create synthetic data\n",
    "data = torch.randn(100, 10)\n",
    "targets = torch.randn(100, 1)\n",
    "\n",
    "# Non-vectorized training loop with inefficiencies\n",
    "def train_inefficient(model, data, targets, epochs=5):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Create DataLoader inside the loop (inefficient)\n",
    "        dataset = torch.utils.data.TensorDataset(data, targets)\n",
    "        train_loader = DataLoader(dataset, batch_size=32)\n",
    "        \n",
    "        # Process individual samples instead of batches (inefficient)\n",
    "        for i in range(len(data)):\n",
    "            # Non-vectorized operation\n",
    "            output = model(data[i:i+1])\n",
    "            loss = criterion(output, targets[i:i+1])\n",
    "            \n",
    "            # Call backward but forget to call optimizer.step()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Unnecessary .item() call for printing\n",
    "            print(f\"Epoch {epoch}, Sample {i}, Loss: {loss.item()}\")\n",
    "            \n",
    "# Create model and train\n",
    "model = SimpleModel()\n",
    "train_inefficient(model, data, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b1d896",
   "metadata": {},
   "source": [
    "## Efficient Implementation\n",
    "\n",
    "Here's how the same code should be written for better efficiency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2f23f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ml_advisor\n",
    "\n",
    "# Efficient implementation\n",
    "def train_efficient(model, data, targets, epochs=5):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    \n",
    "    # Create DataLoader outside the loop\n",
    "    dataset = torch.utils.data.TensorDataset(data, targets)\n",
    "    train_loader = DataLoader(dataset, batch_size=32)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        # Process batches of data\n",
    "        for batch_data, batch_targets in train_loader:\n",
    "            # Vectorized operation on batches\n",
    "            outputs = model(batch_data)\n",
    "            loss = criterion(outputs, batch_targets)\n",
    "            \n",
    "            # Proper optimization steps\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Accumulate loss without unnecessary .item() calls\n",
    "            epoch_loss += loss\n",
    "            \n",
    "        # Print only once per epoch\n",
    "        print(f\"Epoch {epoch}, Loss: {epoch_loss/len(train_loader)}\")\n",
    "        \n",
    "# Create model and train efficiently\n",
    "model = SimpleModel()\n",
    "train_efficient(model, data, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4b60c1",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The ML Advisor extension can help identify common inefficiencies in machine learning code, including:\n",
    "\n",
    "1. Non-vectorized operations in training loops\n",
    "2. Recreation of DataLoaders inside training loops\n",
    "3. Incomplete optimization steps (missing optimizer.step() after backward())\n",
    "4. Unnecessary .item() calls\n",
    "\n",
    "By avoiding these inefficiencies, you can improve the performance and correctness of your ML code."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
